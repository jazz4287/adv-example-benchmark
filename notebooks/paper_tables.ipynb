{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Condensed Tables first\n",
    "## Helpers\n"
   ],
   "id": "37c685fa956c43d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:06:49.592062Z",
     "start_time": "2024-11-22T16:06:49.590514Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a0fb22d43084ec75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "datasets = [\"cifar10\", \"imagenet\"]\n",
    "defendeds = [True, False]\n",
    "threat_model = \"Linf\"\n",
    "experiments = [\"base\", \"worst_defended\", \"best_defended\"]\n",
    "import csv\n",
    "import numpy as np\n",
    "from global_settings import surrogate_models_used, not_working_models, worst_and_best_model_names, undefended_model_names\n",
    "from run import attack_list\n",
    "from utils import get_not_working_model_list\n",
    "\n",
    "\n",
    "def get_models_to_ignore(dataset):\n",
    "    # we take the models that are used as surrogate at any point in any experiment that we present and remove them from the aggregate computation\n",
    "    models_to_ignore = []\n",
    "    for key in surrogate_models_used.keys():\n",
    "        if dataset in surrogate_models_used[key].keys():\n",
    "            for val in surrogate_models_used[key][dataset]:\n",
    "                if val not in models_to_ignore:\n",
    "                    models_to_ignore.append(val)\n",
    "                    \n",
    "    if dataset in not_working_models.keys():\n",
    "        for key in not_working_models[dataset].keys():\n",
    "            for key_2 in not_working_models[dataset][key].keys():\n",
    "                for val in not_working_models[dataset][key][key_2]:\n",
    "                    if val not in models_to_ignore:\n",
    "                        models_to_ignore.append(val)\n",
    "\n",
    "    if dataset in worst_and_best_model_names.keys():\n",
    "        for val in worst_and_best_model_names[dataset].values():\n",
    "            if val not in models_to_ignore:\n",
    "                models_to_ignore.append(val)\n",
    "\n",
    "    return models_to_ignore\n",
    "\n",
    "\n",
    "def experiment_mapper( e: str):\n",
    "    if e.startswith(\"worst_defended\"):\n",
    "        return \"worst_defended\"\n",
    "    elif e.startswith(\"best_defended\"):\n",
    "        return \"best_defended\"\n",
    "    else:\n",
    "        return \"base\"\n",
    "    \n",
    "def get_base_name(name):\n",
    "    space_parsed = name.split(\" \")\n",
    "    combination_parsed = [a.split(\"-\") for a in space_parsed]\n",
    "    combination_parsed = [a for b in combination_parsed for a in b]\n",
    "    combination_parsed = [a.split(\"+\") for a in combination_parsed]\n",
    "    combination_parsed = [a for b in combination_parsed for a in b]\n",
    "    combination_parsed = [a.lower() for a in combination_parsed]\n",
    "    attacks_found = []\n",
    "    for a in attack_list:\n",
    "        if a in combination_parsed:\n",
    "            attacks_found.append(a)     \n",
    "    if len(attacks_found) == 0:\n",
    "        if \"vni\" in combination_parsed and \"fgsm\" in combination_parsed:\n",
    "            attacks_found.append(\"vnifgsm\")\n",
    "    assert len(attacks_found) == 1, print(attacks_found, combination_parsed)\n",
    "    variant = [c for c in name.split(\"(\") if c.endswith(\")\")]\n",
    "    if len(variant) == 1:\n",
    "        return attacks_found[0].upper() + \" (\" +variant[0]\n",
    "    elif len(variant) == 0:\n",
    "        return attacks_found[0].upper()\n",
    "    else:\n",
    "        raise AttributeError\n",
    "    \n",
    "def get_undefended_models(dataset):\n",
    "    undefended_models = undefended_model_names[dataset] + [(\"Standard\" if dataset == \"cifar10\" else \"Standard_R50\")]   # robustbench includes an undefended model\n",
    "    return undefended_models\n"
   ],
   "id": "a33b14fd377df370",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Condensed Tables\n",
    "\n",
    "For each attack variant, for each dataset, we compute the average ASR of each experiment as well as the count of models included in the aggregation for verification purposes.\n",
    "Since now the support for all attacks is the same, we can actually compute the degradation (diff) as the difference of the averages instead of having to compute the average of the differences (they are the same in this specific case)"
   ],
   "id": "ff59ec65f02dca08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T19:18:24.568011Z",
     "start_time": "2024-12-10T19:18:24.523386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"../results/results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "aggregate_results = {}\n",
    "counts = {}\n",
    "# benign_models = {}\n",
    "\n",
    "for experiment in experiments:\n",
    "    aggregate_results[experiment] = {}\n",
    "    counts[experiment] = {}\n",
    "    for dataset in datasets:\n",
    "        undefended_models = get_undefended_models(dataset)\n",
    "        aggregate_results[experiment][dataset] = {}\n",
    "        counts[experiment][dataset] = {}\n",
    "        models_to_ignore = get_models_to_ignore(dataset)\n",
    "        for defended in defendeds:\n",
    "            aggregate_results[experiment][dataset][defended] = {}\n",
    "            counts[experiment][dataset][defended] = {}\n",
    "            relevant_results = results[dataset][threat_model]\n",
    "            attacks = list(relevant_results.keys())\n",
    "            for attack in attacks:\n",
    "                aggregate_results[experiment][dataset][defended][attack] = {\"adv_acc\": 0}\n",
    "                counts[experiment][dataset][defended][attack] = 0\n",
    "                for model in relevant_results[attack].keys():\n",
    "                    if model not in models_to_ignore:\n",
    "                        if (defended and (model not in undefended_models)) or ((not defended) and (model in undefended_models)):\n",
    "                            for exp, val in relevant_results[attack][model].items():\n",
    "                                if experiment_mapper(exp) == experiment:\n",
    "                                    aggregate_results[experiment][dataset][defended][attack][\"adv_acc\"] +=  val\n",
    "                                    counts[experiment][dataset][defended][attack] += 1\n",
    "                                    break\n",
    "                if counts[experiment][dataset][defended][attack] == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    aggregate_results[experiment][dataset][defended][attack][\"adv_acc\"] /= counts[experiment][dataset][defended][attack]\n",
    "            \n",
    "# we wait until the first pass to do this in case benign is not the first key\n",
    "# we also construct the csv\n",
    "csv_table = [[\"Dataset\", \"Defended\", \"Experiment\", \"Attack\", \"Deg\", \"Adv Acc\", \"Count\"]]\n",
    "for experiment in experiments:\n",
    "    for dataset in datasets:\n",
    "        for defended in defendeds:\n",
    "            relevant_results = results[dataset][threat_model]\n",
    "            attacks = list(relevant_results.keys())\n",
    "            for attack in attacks:\n",
    "                if counts[experiment][dataset][defended][attack] != 0:\n",
    "                    # we use the base benign performance because the others are meaningless \n",
    "                    # if attack != \"Square\" and attack != \"BIA (B)\":\n",
    "                    assert counts[experiment][dataset][defended][attack] == counts[\"base\"][dataset][defended][\"Benign\"], f\"{attack} {dataset} {defended} {experiment} {counts[experiment][dataset][defended][attack]} {counts[\"base\"][dataset][defended][\"Benign\"]}\"\n",
    "                    aggregate_results[experiment][dataset][defended][attack][\"deg\"] = aggregate_results[\"base\"][dataset][defended][\"Benign\"][\"adv_acc\"] - aggregate_results[experiment][dataset][defended][attack][\"adv_acc\"]\n",
    "                    csv_table.append([dataset, defended, experiment, attack, f\"{aggregate_results[experiment][dataset][defended][attack][\"deg\"]*100:.2f}\", f\"{aggregate_results[experiment][dataset][defended][attack][\"adv_acc\"]*100:.2f}\", counts[experiment][dataset][defended][attack]])\n",
    "            # print(experiment, dataset, defended)\n",
    "            # print(aggregate_results[experiment][dataset][defended])\n",
    "            # print(counts[experiment][dataset][defended])\n",
    "            \n",
    "with open(\"outputs/results.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(csv_table)\n",
    "\n",
    "            \n",
    "# print(counts)\n",
    "# since \n",
    "                \n",
    "            "
   ],
   "id": "58c1464dc8cb9b08",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T16:06:54.810188Z",
     "start_time": "2024-11-22T16:06:54.808656Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "62a6dc972c9dbf2e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
